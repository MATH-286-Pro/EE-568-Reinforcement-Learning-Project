{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5e72df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514fce1",
   "metadata": {},
   "source": [
    "## **User Define Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5849e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建类\n",
    "class Data_Class:\n",
    "\n",
    "    # 内部嵌套类\n",
    "    class Trajectory_Class:\n",
    "        def __init__(self, traj_series):\n",
    "            self.traj_list = traj_series\n",
    "            self.length = len(traj_series)\n",
    "\n",
    "        def get_single_traj(self, index):\n",
    "            return json.loads(self.traj_list[index])\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        # 原始数据\n",
    "        self.trajs_prefer_list = []\n",
    "        self.trajs_reject_list = []\n",
    "\n",
    "        # 处理数据\n",
    "        self.traj_prefer_list_list_tensor = []\n",
    "        self.traj_reject_list_list_tensor = []\n",
    "\n",
    "        # 启动函数\n",
    "        self.load_data(path)\n",
    "        self.convert(self.trajs_prefer_list, self.traj_prefer_list_list_tensor) # 数据转换\n",
    "        self.convert(self.trajs_reject_list, self.traj_reject_list_list_tensor)\n",
    "        print(\"Data loaded successfully\")\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        self.trajs_prefer_list = Data_Class.Trajectory_Class(data['preferred'])   # list 数据\n",
    "        self.trajs_reject_list = Data_Class.Trajectory_Class(data['rejected'])    # list 数据\n",
    "\n",
    "    def convert(self,\n",
    "                list_json: Trajectory_Class,\n",
    "                traj_list_list_tensor):\n",
    "\n",
    "        # 获取第0条轨迹的第0时刻样本来确定维度\n",
    "        sample = list_json.get_single_traj(0)[0]\n",
    "        state0 = np.array(sample['state'])\n",
    "        action0 = np.array(sample['action'])\n",
    "\n",
    "        # 获取 state action 维度\n",
    "        self.dim_state = state0.size if state0.ndim == 0 else state0.shape[0]\n",
    "        self.dim_action = action0.size if action0.ndim == 0 else action0.shape[0]\n",
    "\n",
    "        # 数据批量转换 tensor\n",
    "        for idx in range(list_json.length):\n",
    "            traj = list_json.get_single_traj(idx)\n",
    "            states, actions = [], []\n",
    "\n",
    "            for time_i in traj:\n",
    "                # 转换为 numpy，然后 torch tensor\n",
    "                state_np = np.array(time_i['state'])\n",
    "                action_np = np.array(time_i['action'])\n",
    "\n",
    "                state_t = torch.from_numpy(state_np).float()\n",
    "                action_t = torch.from_numpy(action_np).float()\n",
    "\n",
    "                # 如果是一维标量，要展开成长度1向量\n",
    "                state_t = state_t.view(-1)\n",
    "                action_t = action_t.view(-1)\n",
    "\n",
    "                states.append(state_t)\n",
    "                actions.append(action_t)\n",
    "\n",
    "            # 将列表堆成张量 [L_i, dim]\n",
    "            states_tensor = torch.stack(states, dim=0)\n",
    "            actions_tensor = torch.stack(actions, dim=0)\n",
    "\n",
    "            # 将每条轨迹作为一个元组 (states, actions) 添加到列表中\n",
    "            traj_list_list_tensor.append((states_tensor, actions_tensor))\n",
    "\n",
    "# ——— 数据集与加载器 ———\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, pref, rej, gamma):\n",
    "        assert len(pref) == len(rej)\n",
    "        self.pref = pref\n",
    "        self.rej = rej\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pref)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (*self.pref[idx], *self.rej[idx])\n",
    "\n",
    "# 创建 MLP 打分模型\n",
    "class RewardMLP(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim + a_dim, hidden_dim),  # 这里在构造神经网络\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        # s: [L_i, s_dim], a: [L_i, a_dim]\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b8606d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "200 200\n",
      "200 144\n",
      "200 180\n",
      "\n",
      "200 200\n",
      "200 144\n"
     ]
    }
   ],
   "source": [
    "# ———— 加载数据 ————\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "print(len(Data.trajs_prefer_list.get_single_traj(0)),  len(Data.trajs_reject_list.get_single_traj(0)))\n",
    "print(len(Data.trajs_prefer_list.get_single_traj(1)),  len(Data.trajs_reject_list.get_single_traj(1)))\n",
    "print(len(Data.trajs_prefer_list.get_single_traj(2)),  len(Data.trajs_reject_list.get_single_traj(2)))\n",
    "print(\"\")\n",
    "print(len(Data.traj_prefer_list_list_tensor[0][0]), len(Data.traj_reject_list_list_tensor[0][0]))\n",
    "print(len(Data.traj_prefer_list_list_tensor[1][0]), len(Data.traj_reject_list_list_tensor[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eeca22",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b87a9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Epoch 1/50 — Avg Loss: 0.5030\n",
      "Epoch 2/50 — Avg Loss: 0.3736\n",
      "Epoch 3/50 — Avg Loss: 0.3204\n",
      "Epoch 4/50 — Avg Loss: 0.2934\n",
      "Epoch 5/50 — Avg Loss: 0.2803\n",
      "Epoch 6/50 — Avg Loss: 0.2708\n",
      "Epoch 7/50 — Avg Loss: 0.2642\n",
      "Epoch 8/50 — Avg Loss: 0.2583\n",
      "Epoch 9/50 — Avg Loss: 0.2534\n",
      "Epoch 10/50 — Avg Loss: 0.2486\n",
      "Epoch 11/50 — Avg Loss: 0.2452\n",
      "Epoch 12/50 — Avg Loss: 0.2420\n",
      "Epoch 13/50 — Avg Loss: 0.2389\n",
      "Epoch 14/50 — Avg Loss: 0.2363\n",
      "Epoch 15/50 — Avg Loss: 0.2346\n",
      "Epoch 16/50 — Avg Loss: 0.2330\n",
      "Epoch 17/50 — Avg Loss: 0.2311\n",
      "Epoch 18/50 — Avg Loss: 0.2293\n",
      "Epoch 19/50 — Avg Loss: 0.2277\n",
      "Epoch 20/50 — Avg Loss: 0.2264\n",
      "Epoch 21/50 — Avg Loss: 0.2246\n",
      "Epoch 22/50 — Avg Loss: 0.2240\n",
      "Epoch 23/50 — Avg Loss: 0.2226\n",
      "Epoch 24/50 — Avg Loss: 0.2212\n",
      "Epoch 25/50 — Avg Loss: 0.2211\n",
      "Epoch 26/50 — Avg Loss: 0.2179\n",
      "Epoch 27/50 — Avg Loss: 0.2195\n",
      "Epoch 28/50 — Avg Loss: 0.2177\n",
      "Epoch 29/50 — Avg Loss: 0.2173\n",
      "Epoch 30/50 — Avg Loss: 0.2158\n",
      "Epoch 31/50 — Avg Loss: 0.2142\n",
      "Epoch 32/50 — Avg Loss: 0.2127\n",
      "Epoch 33/50 — Avg Loss: 0.2118\n",
      "Epoch 34/50 — Avg Loss: 0.2113\n",
      "Epoch 35/50 — Avg Loss: 0.2101\n",
      "Epoch 36/50 — Avg Loss: 0.2094\n",
      "Epoch 37/50 — Avg Loss: 0.2091\n",
      "Epoch 38/50 — Avg Loss: 0.2080\n",
      "Epoch 39/50 — Avg Loss: 0.2071\n",
      "Epoch 40/50 — Avg Loss: 0.2060\n",
      "Epoch 41/50 — Avg Loss: 0.2058\n",
      "Epoch 42/50 — Avg Loss: 0.2049\n",
      "Epoch 43/50 — Avg Loss: 0.2045\n",
      "Epoch 44/50 — Avg Loss: 0.2060\n",
      "Epoch 45/50 — Avg Loss: 0.2043\n",
      "Epoch 46/50 — Avg Loss: 0.2035\n",
      "Epoch 47/50 — Avg Loss: 0.2020\n",
      "Epoch 48/50 — Avg Loss: 0.2024\n",
      "Epoch 49/50 — Avg Loss: 0.2002\n",
      "Epoch 50/50 — Avg Loss: 0.1996\n",
      "🎉 合成数据上的奖励模型训练完成！\n",
      "模型已保存到 reward_net.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ———— 超参数 ————\n",
    "num_pairs = 200    # 偏好对数量\n",
    "T = 50             # 期望最大轨迹长度（用于评估或其他需求）\n",
    "s_dim = 4         # 状态维度 [角度, 角速度, 小车位置, 小车速度]\n",
    "a_dim = 1         # 动作维度（推力）\n",
    "gamma = 0.99      # 折扣因子\n",
    "lr = 1e-4         # 学习率\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "# ———— 加载数据 ————\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "# 自定义 collate_fn，保留变长序列\n",
    "def variable_collate(batch):\n",
    "    # batch: List of tuples (s_pref, a_pref, s_rej, a_rej)\n",
    "    s_pf, a_pf, s_rj, a_rj = zip(*batch)\n",
    "    return list(s_pf), list(a_pf), list(s_rj), list(a_rj)\n",
    "\n",
    "# 准备训练\n",
    "dataset = PreferenceDataset(\n",
    "    Data.traj_prefer_list_list_tensor,\n",
    "    Data.traj_reject_list_list_tensor,\n",
    "    gamma\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=variable_collate\n",
    ")\n",
    "\n",
    "reward_net = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64)\n",
    "optimizer  = optim.Adam(reward_net.parameters(), lr=lr)\n",
    "loss_fn    = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ——— 训练循环 ———\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for s_pref_list, a_pref_list, s_rej_list, a_rej_list in loader:\n",
    "        R_pref_batch = []\n",
    "        R_rej_batch  = []\n",
    "\n",
    "        # 计算 prefer 轨迹的回报\n",
    "        for s_pf, a_pf in zip(s_pref_list, a_pref_list):\n",
    "            r_pf = reward_net(s_pf, a_pf)           # [L_i]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_pf.size(0))], device=r_pf.device)\n",
    "            R_pref_batch.append((r_pf * discounts).sum())\n",
    "\n",
    "        # 计算 reject 轨迹的回报\n",
    "        for s_rj, a_rj in zip(s_rej_list, a_rej_list):\n",
    "            r_rj = reward_net(s_rj, a_rj)          # [L_j]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))], device=r_rj.device)\n",
    "            R_rej_batch.append((r_rj * discounts).sum())\n",
    "\n",
    "        R_pref = torch.stack(R_pref_batch)\n",
    "        R_rej = torch.stack(R_rej_batch)\n",
    "\n",
    "        logits = R_pref - R_rej\n",
    "        targets = torch.ones_like(logits)        # pref 应得更高分\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(R_pref_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} — Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"🎉 合成数据上的奖励模型训练完成！\")\n",
    "\n",
    "# ——— 保存模型 ———\n",
    "torch.save(reward_net.state_dict(), 'reward_net.pth')\n",
    "print(\"模型已保存到 reward_net.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6bec",
   "metadata": {},
   "source": [
    "## **Load Grading Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9ec08b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载并准备好进行推理\n",
      "折扣后总回报： tensor(18.6901)\n"
     ]
    }
   ],
   "source": [
    "# 假设你已有一条轨迹的原始 JSON 数据 traj_json\n",
    "traj_json = Data.trajs_prefer_list.get_single_traj(2)\n",
    "traj_json = Data.trajs_reject_list.get_single_traj(2)\n",
    "\n",
    "# 把它转成张量\n",
    "states  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                       for step in traj_json], dim=0)  # [L, s_dim]\n",
    "actions = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                       for step in traj_json], dim=0)  # [L, a_dim]\n",
    "\n",
    "\n",
    "\n",
    "# ——— 加载模型示例 ———\n",
    "reward_net_loaded = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64)\n",
    "reward_net_loaded.load_state_dict(torch.load('reward_net.pth', weights_only=True))\n",
    "\n",
    "# 切到推理模式，并关闭梯度\n",
    "reward_net_loaded.eval()\n",
    "print(\"加载并准备好进行推理\")\n",
    "with torch.no_grad():\n",
    "    per_step_rewards = reward_net_loaded(states, actions)  # 张量形状 [L]\n",
    "\n",
    "# 如果你想要轨迹的总折扣回报：\n",
    "discounts = torch.tensor([gamma**t for t in range(per_step_rewards.size(0))])\n",
    "total_return = (per_step_rewards * discounts).sum()\n",
    "# print(\"每步奖励：\", per_step_rewards)\n",
    "print(\"折扣后总回报：\", total_return)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
