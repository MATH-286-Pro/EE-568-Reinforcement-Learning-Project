{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e72df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514fce1",
   "metadata": {},
   "source": [
    "## **User Define Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5849e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建类\n",
    "class Data_Class:\n",
    "\n",
    "    # 内部嵌套类\n",
    "    class Trajectory_Class:\n",
    "        def __init__(self, traj_series):\n",
    "            self.traj_list = traj_series\n",
    "            self.length = len(traj_series)\n",
    "\n",
    "        def get_single_traj(self, index):\n",
    "            return json.loads(self.traj_list[index])\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        # 原始数据\n",
    "        self.trajs_prefer_list = []\n",
    "        self.trajs_reject_list = []\n",
    "\n",
    "        # 处理数据\n",
    "        self.traj_prefer_list_list_tensor = []\n",
    "        self.traj_reject_list_list_tensor = []\n",
    "\n",
    "        # 启动函数\n",
    "        self.load_data(path)\n",
    "        self.convert(self.trajs_prefer_list, self.traj_prefer_list_list_tensor) # 数据转换\n",
    "        self.convert(self.trajs_reject_list, self.traj_reject_list_list_tensor)\n",
    "        print(\"Data loaded successfully\")\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        self.trajs_prefer_list = Data_Class.Trajectory_Class(data['preferred'])   # list 数据\n",
    "        self.trajs_reject_list = Data_Class.Trajectory_Class(data['rejected'])    # list 数据\n",
    "\n",
    "    def convert(self,\n",
    "                list_json: Trajectory_Class,\n",
    "                traj_list_list_tensor):\n",
    "\n",
    "        # 获取第0条轨迹的第0时刻样本来确定维度\n",
    "        sample = list_json.get_single_traj(0)[0]\n",
    "        state0 = np.array(sample['state'])\n",
    "        action0 = np.array(sample['action'])\n",
    "\n",
    "        # 获取 state action 维度\n",
    "        self.dim_state = state0.size if state0.ndim == 0 else state0.shape[0]\n",
    "        self.dim_action = action0.size if action0.ndim == 0 else action0.shape[0]\n",
    "\n",
    "        # 数据批量转换 tensor\n",
    "        for idx in range(list_json.length):\n",
    "            traj = list_json.get_single_traj(idx)\n",
    "            states, actions = [], []\n",
    "\n",
    "            for time_i in traj:\n",
    "                # 转换为 numpy，然后 torch tensor\n",
    "                state_np = np.array(time_i['state'])\n",
    "                action_np = np.array(time_i['action'])\n",
    "\n",
    "                state_t = torch.from_numpy(state_np).float()\n",
    "                action_t = torch.from_numpy(action_np).float()\n",
    "\n",
    "                # 如果是一维标量，要展开成长度1向量\n",
    "                state_t = state_t.view(-1)\n",
    "                action_t = action_t.view(-1)\n",
    "\n",
    "                states.append(state_t)\n",
    "                actions.append(action_t)\n",
    "\n",
    "            # 将列表堆成张量 [L_i, dim]\n",
    "            states_tensor = torch.stack(states, dim=0)\n",
    "            actions_tensor = torch.stack(actions, dim=0)\n",
    "\n",
    "            # 将每条轨迹作为一个元组 (states, actions) 添加到列表中\n",
    "            traj_list_list_tensor.append((states_tensor, actions_tensor))\n",
    "\n",
    "# ——— 数据集与加载器 ———\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, pref, rej, gamma):\n",
    "        assert len(pref) == len(rej)\n",
    "        self.pref = pref\n",
    "        self.rej = rej\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pref)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (*self.pref[idx], *self.rej[idx])\n",
    "\n",
    "# 创建 MLP 打分模型 #0000FF\n",
    "class RewardMLP(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim + a_dim, hidden_dim),  # 这里在构造神经网络\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        # s: [L_i, s_dim], a: [L_i, a_dim]\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8606d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "0 200 200\n",
      "1 200 144\n",
      "2 200 180\n",
      "3 200 194\n",
      "4 200 96\n",
      "5 200 125\n",
      "6 200 108\n",
      "7 200 106\n",
      "8 200 166\n",
      "9 173 97\n"
     ]
    }
   ],
   "source": [
    "# ———— 加载数据 ————\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, len(Data.trajs_prefer_list.get_single_traj(i)), len(Data.trajs_reject_list.get_single_traj(i)))\n",
    "\n",
    "# print(len(Data.trajs_prefer_list.get_single_traj(0)),  len(Data.trajs_reject_list.get_single_traj(0)))\n",
    "# print(len(Data.trajs_prefer_list.get_single_traj(1)),  len(Data.trajs_reject_list.get_single_traj(1)))\n",
    "# print(len(Data.trajs_prefer_list.get_single_traj(2)),  len(Data.trajs_reject_list.get_single_traj(2)))\n",
    "\n",
    "# print(\"\")\n",
    "# print(len(Data.traj_prefer_list_list_tensor[0][0]), len(Data.traj_reject_list_list_tensor[0][0]))\n",
    "# print(len(Data.traj_prefer_list_list_tensor[1][0]), len(Data.traj_reject_list_list_tensor[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eeca22",
   "metadata": {},
   "source": [
    "## **Reward Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b87a9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Epoch 1/50 — Avg Loss: 1.8811\n",
      "Epoch 2/50 — Avg Loss: 1.5743\n",
      "Epoch 3/50 — Avg Loss: 1.3052\n",
      "Epoch 4/50 — Avg Loss: 1.0893\n",
      "Epoch 5/50 — Avg Loss: 0.9303\n",
      "Epoch 6/50 — Avg Loss: 0.7888\n",
      "Epoch 7/50 — Avg Loss: 0.6872\n",
      "Epoch 8/50 — Avg Loss: 0.5927\n",
      "Epoch 9/50 — Avg Loss: 0.5236\n",
      "Epoch 10/50 — Avg Loss: 0.4684\n",
      "Epoch 11/50 — Avg Loss: 0.4259\n",
      "Epoch 12/50 — Avg Loss: 0.3925\n",
      "Epoch 13/50 — Avg Loss: 0.3677\n",
      "Epoch 14/50 — Avg Loss: 0.3490\n",
      "Epoch 15/50 — Avg Loss: 0.3347\n",
      "Epoch 16/50 — Avg Loss: 0.3233\n",
      "Epoch 17/50 — Avg Loss: 0.3135\n",
      "Epoch 18/50 — Avg Loss: 0.3075\n",
      "Epoch 19/50 — Avg Loss: 0.3004\n",
      "Epoch 20/50 — Avg Loss: 0.2953\n",
      "Epoch 21/50 — Avg Loss: 0.2907\n",
      "Epoch 22/50 — Avg Loss: 0.2864\n",
      "Epoch 23/50 — Avg Loss: 0.2825\n",
      "Epoch 24/50 — Avg Loss: 0.2795\n",
      "Epoch 25/50 — Avg Loss: 0.2760\n",
      "Epoch 26/50 — Avg Loss: 0.2737\n",
      "Epoch 27/50 — Avg Loss: 0.2711\n",
      "Epoch 28/50 — Avg Loss: 0.2686\n",
      "Epoch 29/50 — Avg Loss: 0.2666\n",
      "Epoch 30/50 — Avg Loss: 0.2649\n",
      "Epoch 31/50 — Avg Loss: 0.2622\n",
      "Epoch 32/50 — Avg Loss: 0.2613\n",
      "Epoch 33/50 — Avg Loss: 0.2587\n",
      "Epoch 34/50 — Avg Loss: 0.2587\n",
      "Epoch 35/50 — Avg Loss: 0.2568\n",
      "Epoch 36/50 — Avg Loss: 0.2547\n",
      "Epoch 37/50 — Avg Loss: 0.2533\n",
      "Epoch 38/50 — Avg Loss: 0.2521\n",
      "Epoch 39/50 — Avg Loss: 0.2505\n",
      "Epoch 40/50 — Avg Loss: 0.2493\n",
      "Epoch 41/50 — Avg Loss: 0.2488\n",
      "Epoch 42/50 — Avg Loss: 0.2470\n",
      "Epoch 43/50 — Avg Loss: 0.2460\n",
      "Epoch 44/50 — Avg Loss: 0.2455\n",
      "Epoch 45/50 — Avg Loss: 0.2437\n",
      "Epoch 46/50 — Avg Loss: 0.2430\n",
      "Epoch 47/50 — Avg Loss: 0.2415\n",
      "Epoch 48/50 — Avg Loss: 0.2406\n",
      "Epoch 49/50 — Avg Loss: 0.2403\n",
      "Epoch 50/50 — Avg Loss: 0.2395\n",
      "🎉 合成数据上的奖励模型训练完成！\n",
      "模型已保存到 reward_net.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ———— 超参数 ————\n",
    "num_pairs = 200    # 偏好对数量\n",
    "T = 50             # 期望最大轨迹长度（用于评估或其他需求）\n",
    "s_dim = 4         # 状态维度 [角度, 角速度, 小车位置, 小车速度]\n",
    "a_dim = 1         # 动作维度（推力）\n",
    "gamma = 0.99      # 折扣因子\n",
    "lr = 1e-4         # 学习率\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "# ———— 加载数据 ————\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "# 自定义 collate_fn，保留变长序列\n",
    "def variable_collate(batch):\n",
    "    # batch: List of tuples (s_pref, a_pref, s_rej, a_rej)\n",
    "    s_pf, a_pf, s_rj, a_rj = zip(*batch)\n",
    "    return list(s_pf), list(a_pf), list(s_rj), list(a_rj)\n",
    "\n",
    "# 准备训练\n",
    "dataset = PreferenceDataset(\n",
    "    Data.traj_prefer_list_list_tensor,\n",
    "    Data.traj_reject_list_list_tensor,\n",
    "    gamma\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=variable_collate\n",
    ")\n",
    "\n",
    "reward_net = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64) # 实例化 神经网络 MLP\n",
    "optimizer  = optim.Adam(reward_net.parameters(), lr=lr)\n",
    "loss_fn    = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ——— 训练循环 ———\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for s_pref_list, a_pref_list, s_rej_list, a_rej_list in loader:\n",
    "        R_pref_batch = []\n",
    "        R_rej_batch  = []\n",
    "\n",
    "        # 计算 prefer 轨迹的回报\n",
    "        for s_pf, a_pf in zip(s_pref_list, a_pref_list):\n",
    "            r_pf = reward_net(s_pf, a_pf)           # [L_i]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_pf.size(0))], device=r_pf.device)\n",
    "            R_pref_batch.append((r_pf * discounts).sum())\n",
    "\n",
    "        # 计算 reject 轨迹的回报\n",
    "        for s_rj, a_rj in zip(s_rej_list, a_rej_list):\n",
    "            r_rj = reward_net(s_rj, a_rj)          # [L_j]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))], device=r_rj.device)\n",
    "            R_rej_batch.append((r_rj * discounts).sum())\n",
    "\n",
    "        R_pref = torch.stack(R_pref_batch)\n",
    "        R_rej = torch.stack(R_rej_batch)\n",
    "\n",
    "        logits = R_pref - R_rej\n",
    "        targets = torch.ones_like(logits)        # pref 应得更高分\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(R_pref_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} — Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"🎉 合成数据上的奖励模型训练完成！\")\n",
    "\n",
    "# ——— 保存模型 ———\n",
    "torch.save(reward_net.state_dict(), 'reward_net.pth')\n",
    "print(\"模型已保存到 reward_net.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6bec",
   "metadata": {},
   "source": [
    "## **Reward Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "076e2acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载并准备好进行推理\n",
      "0 tensor(10.9267) tensor(9.9196)\n",
      "1 tensor(9.7878) tensor(7.5548)\n",
      "2 tensor(11.4603) tensor(9.7593)\n",
      "3 tensor(10.9521) tensor(9.6708)\n",
      "4 tensor(11.2456) tensor(5.3581)\n",
      "5 tensor(11.0701) tensor(6.5134)\n",
      "6 tensor(10.5024) tensor(5.3424)\n",
      "7 tensor(10.8763) tensor(5.1974)\n",
      "8 tensor(10.2842) tensor(9.6710)\n",
      "9 tensor(10.4395) tensor(6.1724)\n"
     ]
    }
   ],
   "source": [
    "# ——— 加载模型示例 ———\n",
    "reward_net_loaded = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64)\n",
    "reward_net_loaded.load_state_dict(torch.load('reward_net.pth', weights_only=True))\n",
    "\n",
    "# 切到推理模式，并关闭梯度\n",
    "reward_net_loaded.eval()\n",
    "print(\"加载并准备好进行推理\")\n",
    "\n",
    "# 加载数据\n",
    "for i in range(10):\n",
    "    traj_prefer_json = Data.trajs_prefer_list.get_single_traj(i)\n",
    "    traj_reject_json = Data.trajs_reject_list.get_single_traj(i)\n",
    "\n",
    "    # 把它转成张量\n",
    "    states_prefer  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                  for step in traj_prefer_json], dim=0)  # [L, s_dim]\n",
    "    actions_prefer = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_prefer_json], dim=0)  # [L, a_dim]\n",
    "    states_reject  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, s_dim]\n",
    "    actions_reject = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, a_dim]\n",
    "\n",
    "    # 计算 prefer 轨迹的回报\n",
    "    with torch.no_grad():\n",
    "        r_pref = reward_net_loaded(states_prefer, actions_prefer)           # [L_i]\n",
    "\n",
    "    # 计算总回报\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_pref.size(0))])\n",
    "    total_return_prefer = (r_pref * discounts).sum()\n",
    "\n",
    "    # 计算 reject 轨迹的回报\n",
    "    with torch.no_grad():\n",
    "        r_rj = reward_net_loaded(states_reject, actions_reject)          # [L_j]\n",
    "\n",
    "    # 计算总回报\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))])\n",
    "    total_return_reject = (r_rj * discounts).sum()\n",
    "\n",
    "    print(i, total_return_prefer, total_return_reject)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79951354",
   "metadata": {},
   "source": [
    "## **RLHF 模型训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41f50c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 2. 定义一个 Wrapper，在 step 里用你的 MLP 计算 reward\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomRewardWrapper\u001b[39;00m(\u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mWrapper):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env, reward_model_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# 2. 定义一个 Wrapper，在 step 里用你的 MLP 计算 reward\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, reward_model_path, device=\"cpu\"):\n",
    "        super().__init__(env)\n",
    "        # 创建并加载你的 MLP 奖励模型\n",
    "\n",
    "        # state 维度\n",
    "        self.dim_state = env.observation_space.shape[0]\n",
    "\n",
    "        # action 维度\n",
    "        try:                  self.dim_action = env.action_space.shape[0]\n",
    "        except IndexError:    self.dim_action = 1\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Ensure the input dimensions match the checkpoint\n",
    "        checkpoint = torch.load(reward_model_path, map_location=device, weights_only=False)\n",
    "        input_dim = checkpoint['net.0.weight'].size(1)  # Extract input size from checkpoint\n",
    "        self.reward_model = RewardMLP(input_dim - self.dim_action, self.dim_action).to(device)\n",
    "        self.reward_model.load_state_dict(checkpoint)\n",
    "        self.reward_model.load_state_dict(torch.load(reward_model_path, map_location=device, weights_only=False))\n",
    "        self.reward_model.eval()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 执行原 env，不用原 reward\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)   \n",
    "        \n",
    "        # 转成 batch 形式再丢给网络\n",
    "        state_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.Discrete):\n",
    "            action_tensor = torch.tensor([action], dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        # 修改 action_tensor 形状\n",
    "        if action_tensor.ndim == 1:\n",
    "            action_tensor = action_tensor.view(1, -1)\n",
    "\n",
    "        # 计算奖励\n",
    "        with torch.no_grad():\n",
    "            reward_tensor = self.reward_model(state_tensor, \n",
    "                                              action_tensor)\n",
    "        reward = reward_tensor.item()\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "def reset(self, **kwargs):\n",
    "    # 接受 seed, options, return_info 等任意参数\n",
    "    return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# 3. 构造 vectorized 环境，并应用自定义 Wrapper\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Log 地址\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_path = os.path.join(\"Training\", current_time)\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "# Reward Model 地址\n",
    "MODEL_PATH = \"reward_net.pth\"\n",
    "\n",
    "# 如果你用 SB3 的 make_vec_env，可以传 wrapper_class\n",
    "vec_env = make_vec_env(\n",
    "    env_id=\"CartPole-v1\",\n",
    "    n_envs=8,\n",
    "    wrapper_class=lambda env: CustomRewardWrapper(env, MODEL_PATH, device=\"cpu\"),\n",
    "    monitor_dir=log_path\n",
    ")\n",
    "# 再加一个 Monitor（记录 episode reward 到文件）\n",
    "vec_env = VecMonitor(vec_env, log_path)\n",
    "\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    n_steps=256,\n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_path\n",
    ")\n",
    "\n",
    "model.learn( \n",
    "    total_timesteps=50000,\n",
    "    # callback=[eval_callback, save_callback]\n",
    ")\n",
    "\n",
    "model.save(os.path.join(log_path, \"model_full_training\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01aac6",
   "metadata": {},
   "source": [
    "## **RLHF 模型测试部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\\2025-05-08_19-59-36\\model_full_training\n",
      "Episode: 1 Score: [500.]\n",
      "Episode: 2 Score: [500.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "pkg_dir = os.path.dirname(cur_dir)\n",
    "\n",
    "if pkg_dir not in sys.path:\n",
    "    sys.path.append(pkg_dir)\n",
    "\n",
    "from Project import tools\n",
    "\n",
    "\n",
    "# 评估模型\n",
    "log_path = \"Training\\\\2025-05-08_19-59-36\"\n",
    "PPO_Model_Path = os.path.join(log_path, \"model_full_training\")\n",
    "tools.test_model(\"PPO\", PPO_Model_Path, n_episodes=2, render = True, record=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
