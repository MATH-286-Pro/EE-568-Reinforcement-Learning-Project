{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e72df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514fce1",
   "metadata": {},
   "source": [
    "## **User Define Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5849e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建类\n",
    "class Data_Class:\n",
    "\n",
    "    # 内部嵌套类\n",
    "    class Trajectory_Class:\n",
    "        def __init__(self, traj_series):\n",
    "            self.traj_list = traj_series\n",
    "            self.length = len(traj_series)\n",
    "\n",
    "        def get_single_traj(self, index):\n",
    "            return json.loads(self.traj_list[index])\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        # 原始数据\n",
    "        self.trajs_prefer_list = []\n",
    "        self.trajs_reject_list = []\n",
    "\n",
    "        # 处理数据\n",
    "        self.traj_prefer_list_list_tensor = []\n",
    "        self.traj_reject_list_list_tensor = []\n",
    "\n",
    "        # 启动函数\n",
    "        self.load_data(path)\n",
    "        self.convert(self.trajs_prefer_list, self.traj_prefer_list_list_tensor) # 数据转换\n",
    "        self.convert(self.trajs_reject_list, self.traj_reject_list_list_tensor)\n",
    "        print(\"Data loaded successfully\")\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        self.trajs_prefer_list = Data_Class.Trajectory_Class(data['preferred'])   # list 数据\n",
    "        self.trajs_reject_list = Data_Class.Trajectory_Class(data['rejected'])    # list 数据\n",
    "\n",
    "    def convert(self,\n",
    "                list_json: Trajectory_Class,\n",
    "                traj_list_list_tensor):\n",
    "\n",
    "        # 获取第0条轨迹的第0时刻样本来确定维度\n",
    "        sample = list_json.get_single_traj(0)[0]\n",
    "        state0 = np.array(sample['state'])\n",
    "        action0 = np.array(sample['action'])\n",
    "\n",
    "        # 获取 state action 维度\n",
    "        self.dim_state = state0.size if state0.ndim == 0 else state0.shape[0]\n",
    "        self.dim_action = action0.size if action0.ndim == 0 else action0.shape[0]\n",
    "\n",
    "        # 数据批量转换 tensor\n",
    "        for idx in range(list_json.length):\n",
    "            traj = list_json.get_single_traj(idx)\n",
    "            states, actions = [], []\n",
    "\n",
    "            for time_i in traj:\n",
    "                # 转换为 numpy，然后 torch tensor\n",
    "                state_np = np.array(time_i['state'])\n",
    "                action_np = np.array(time_i['action'])\n",
    "\n",
    "                state_t = torch.from_numpy(state_np).float()\n",
    "                action_t = torch.from_numpy(action_np).float()\n",
    "\n",
    "                # 如果是一维标量，要展开成长度1向量\n",
    "                state_t = state_t.view(-1)\n",
    "                action_t = action_t.view(-1)\n",
    "\n",
    "                states.append(state_t)\n",
    "                actions.append(action_t)\n",
    "\n",
    "            # 将列表堆成张量 [L_i, dim]\n",
    "            states_tensor = torch.stack(states, dim=0)\n",
    "            actions_tensor = torch.stack(actions, dim=0)\n",
    "\n",
    "            # 将每条轨迹作为一个元组 (states, actions) 添加到列表中\n",
    "            traj_list_list_tensor.append((states_tensor, actions_tensor))\n",
    "\n",
    "# ——— 数据集与加载器 ———\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, pref, rej, gamma):\n",
    "        assert len(pref) == len(rej)\n",
    "        self.pref = pref\n",
    "        self.rej = rej\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pref)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (*self.pref[idx], *self.rej[idx])\n",
    "\n",
    "# 创建 MLP 打分模型 #0000FF\n",
    "class RewardMLP(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim + a_dim, hidden_dim),  # 这里在构造神经网络\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        # s: [L_i, s_dim], a: [L_i, a_dim]\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8606d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "0 200 200\n",
      "1 200 144\n",
      "2 200 180\n",
      "3 200 194\n",
      "4 200 96\n",
      "5 200 125\n",
      "6 200 108\n",
      "7 200 106\n",
      "8 200 166\n",
      "9 173 97\n"
     ]
    }
   ],
   "source": [
    "# ———— 加载数据 ————\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, len(Data.trajs_prefer_list.get_single_traj(i)), len(Data.trajs_reject_list.get_single_traj(i)))\n",
    "\n",
    "# print(len(Data.trajs_prefer_list.get_single_traj(0)),  len(Data.trajs_reject_list.get_single_traj(0)))\n",
    "# print(len(Data.trajs_prefer_list.get_single_traj(1)),  len(Data.trajs_reject_list.get_single_traj(1)))\n",
    "# print(len(Data.trajs_prefer_list.get_single_traj(2)),  len(Data.trajs_reject_list.get_single_traj(2)))\n",
    "\n",
    "# print(\"\")\n",
    "# print(len(Data.traj_prefer_list_list_tensor[0][0]), len(Data.traj_reject_list_list_tensor[0][0]))\n",
    "# print(len(Data.traj_prefer_list_list_tensor[1][0]), len(Data.traj_reject_list_list_tensor[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eeca22",
   "metadata": {},
   "source": [
    "## **Reward Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b87a9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Epoch 1/50 — Avg Loss: 1.8811\n",
      "Epoch 2/50 — Avg Loss: 1.5743\n",
      "Epoch 3/50 — Avg Loss: 1.3052\n",
      "Epoch 4/50 — Avg Loss: 1.0893\n",
      "Epoch 5/50 — Avg Loss: 0.9303\n",
      "Epoch 6/50 — Avg Loss: 0.7888\n",
      "Epoch 7/50 — Avg Loss: 0.6872\n",
      "Epoch 8/50 — Avg Loss: 0.5927\n",
      "Epoch 9/50 — Avg Loss: 0.5236\n",
      "Epoch 10/50 — Avg Loss: 0.4684\n",
      "Epoch 11/50 — Avg Loss: 0.4259\n",
      "Epoch 12/50 — Avg Loss: 0.3925\n",
      "Epoch 13/50 — Avg Loss: 0.3677\n",
      "Epoch 14/50 — Avg Loss: 0.3490\n",
      "Epoch 15/50 — Avg Loss: 0.3347\n",
      "Epoch 16/50 — Avg Loss: 0.3233\n",
      "Epoch 17/50 — Avg Loss: 0.3135\n",
      "Epoch 18/50 — Avg Loss: 0.3075\n",
      "Epoch 19/50 — Avg Loss: 0.3004\n",
      "Epoch 20/50 — Avg Loss: 0.2953\n",
      "Epoch 21/50 — Avg Loss: 0.2907\n",
      "Epoch 22/50 — Avg Loss: 0.2864\n",
      "Epoch 23/50 — Avg Loss: 0.2825\n",
      "Epoch 24/50 — Avg Loss: 0.2795\n",
      "Epoch 25/50 — Avg Loss: 0.2760\n",
      "Epoch 26/50 — Avg Loss: 0.2737\n",
      "Epoch 27/50 — Avg Loss: 0.2711\n",
      "Epoch 28/50 — Avg Loss: 0.2686\n",
      "Epoch 29/50 — Avg Loss: 0.2666\n",
      "Epoch 30/50 — Avg Loss: 0.2649\n",
      "Epoch 31/50 — Avg Loss: 0.2622\n",
      "Epoch 32/50 — Avg Loss: 0.2613\n",
      "Epoch 33/50 — Avg Loss: 0.2587\n",
      "Epoch 34/50 — Avg Loss: 0.2587\n",
      "Epoch 35/50 — Avg Loss: 0.2568\n",
      "Epoch 36/50 — Avg Loss: 0.2547\n",
      "Epoch 37/50 — Avg Loss: 0.2533\n",
      "Epoch 38/50 — Avg Loss: 0.2521\n",
      "Epoch 39/50 — Avg Loss: 0.2505\n",
      "Epoch 40/50 — Avg Loss: 0.2493\n",
      "Epoch 41/50 — Avg Loss: 0.2488\n",
      "Epoch 42/50 — Avg Loss: 0.2470\n",
      "Epoch 43/50 — Avg Loss: 0.2460\n",
      "Epoch 44/50 — Avg Loss: 0.2455\n",
      "Epoch 45/50 — Avg Loss: 0.2437\n",
      "Epoch 46/50 — Avg Loss: 0.2430\n",
      "Epoch 47/50 — Avg Loss: 0.2415\n",
      "Epoch 48/50 — Avg Loss: 0.2406\n",
      "Epoch 49/50 — Avg Loss: 0.2403\n",
      "Epoch 50/50 — Avg Loss: 0.2395\n",
      "🎉 合成数据上的奖励模型训练完成！\n",
      "模型已保存到 reward_net.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ———— 超参数 ————\n",
    "num_pairs = 200    # 偏好对数量\n",
    "T = 50             # 期望最大轨迹长度（用于评估或其他需求）\n",
    "s_dim = 4         # 状态维度 [角度, 角速度, 小车位置, 小车速度]\n",
    "a_dim = 1         # 动作维度（推力）\n",
    "gamma = 0.99      # 折扣因子\n",
    "lr = 1e-4         # 学习率\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "# ———— 加载数据 ————\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "# 自定义 collate_fn，保留变长序列\n",
    "def variable_collate(batch):\n",
    "    # batch: List of tuples (s_pref, a_pref, s_rej, a_rej)\n",
    "    s_pf, a_pf, s_rj, a_rj = zip(*batch)\n",
    "    return list(s_pf), list(a_pf), list(s_rj), list(a_rj)\n",
    "\n",
    "# 准备训练\n",
    "dataset = PreferenceDataset(\n",
    "    Data.traj_prefer_list_list_tensor,\n",
    "    Data.traj_reject_list_list_tensor,\n",
    "    gamma\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=variable_collate\n",
    ")\n",
    "\n",
    "reward_net = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64) # 实例化 神经网络 MLP\n",
    "optimizer  = optim.Adam(reward_net.parameters(), lr=lr)\n",
    "loss_fn    = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ——— 训练循环 ———\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for s_pref_list, a_pref_list, s_rej_list, a_rej_list in loader:\n",
    "        R_pref_batch = []\n",
    "        R_rej_batch  = []\n",
    "\n",
    "        # 计算 prefer 轨迹的回报\n",
    "        for s_pf, a_pf in zip(s_pref_list, a_pref_list):\n",
    "            r_pf = reward_net(s_pf, a_pf)           # [L_i]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_pf.size(0))], device=r_pf.device)\n",
    "            R_pref_batch.append((r_pf * discounts).sum())\n",
    "\n",
    "        # 计算 reject 轨迹的回报\n",
    "        for s_rj, a_rj in zip(s_rej_list, a_rej_list):\n",
    "            r_rj = reward_net(s_rj, a_rj)          # [L_j]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))], device=r_rj.device)\n",
    "            R_rej_batch.append((r_rj * discounts).sum())\n",
    "\n",
    "        R_pref = torch.stack(R_pref_batch)\n",
    "        R_rej = torch.stack(R_rej_batch)\n",
    "\n",
    "        logits = R_pref - R_rej\n",
    "        targets = torch.ones_like(logits)        # pref 应得更高分\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(R_pref_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} — Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"🎉 合成数据上的奖励模型训练完成！\")\n",
    "\n",
    "# ——— 保存模型 ———\n",
    "torch.save(reward_net.state_dict(), 'reward_net.pth')\n",
    "print(\"模型已保存到 reward_net.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6bec",
   "metadata": {},
   "source": [
    "## **Reward Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "076e2acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载并准备好进行推理\n",
      "0 tensor(10.9267) tensor(9.9196)\n",
      "1 tensor(9.7878) tensor(7.5548)\n",
      "2 tensor(11.4603) tensor(9.7593)\n",
      "3 tensor(10.9521) tensor(9.6708)\n",
      "4 tensor(11.2456) tensor(5.3581)\n",
      "5 tensor(11.0701) tensor(6.5134)\n",
      "6 tensor(10.5024) tensor(5.3424)\n",
      "7 tensor(10.8763) tensor(5.1974)\n",
      "8 tensor(10.2842) tensor(9.6710)\n",
      "9 tensor(10.4395) tensor(6.1724)\n"
     ]
    }
   ],
   "source": [
    "# ——— 加载模型示例 ———\n",
    "reward_net_loaded = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64)\n",
    "reward_net_loaded.load_state_dict(torch.load('reward_net.pth', weights_only=True))\n",
    "\n",
    "# 切到推理模式，并关闭梯度\n",
    "reward_net_loaded.eval()\n",
    "print(\"加载并准备好进行推理\")\n",
    "\n",
    "# 加载数据\n",
    "for i in range(10):\n",
    "    traj_prefer_json = Data.trajs_prefer_list.get_single_traj(i)\n",
    "    traj_reject_json = Data.trajs_reject_list.get_single_traj(i)\n",
    "\n",
    "    # 把它转成张量\n",
    "    states_prefer  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                  for step in traj_prefer_json], dim=0)  # [L, s_dim]\n",
    "    actions_prefer = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_prefer_json], dim=0)  # [L, a_dim]\n",
    "    states_reject  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, s_dim]\n",
    "    actions_reject = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, a_dim]\n",
    "\n",
    "    # 计算 prefer 轨迹的回报\n",
    "    with torch.no_grad():\n",
    "        r_pref = reward_net_loaded(states_prefer, actions_prefer)           # [L_i]\n",
    "\n",
    "    # 计算总回报\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_pref.size(0))])\n",
    "    total_return_prefer = (r_pref * discounts).sum()\n",
    "\n",
    "    # 计算 reject 轨迹的回报\n",
    "    with torch.no_grad():\n",
    "        r_rj = reward_net_loaded(states_reject, actions_reject)          # [L_j]\n",
    "\n",
    "    # 计算总回报\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))])\n",
    "    total_return_reject = (r_rj * discounts).sum()\n",
    "\n",
    "    print(i, total_return_prefer, total_return_reject)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79951354",
   "metadata": {},
   "source": [
    "## **RLHF 模型训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f50c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training\\2025-05-08_19-59-36\\PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MATH-286-Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.5     |\n",
      "|    ep_rew_mean     | 2.77     |\n",
      "| time/              |          |\n",
      "|    fps             | 1915     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 24.8         |\n",
      "|    ep_rew_mean          | 3.32         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1087         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114155095 |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.685       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.13         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0197      |\n",
      "|    value_loss           | 0.496        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.8       |\n",
      "|    ep_rew_mean          | 4.37       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 928        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01573616 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.659     |\n",
      "|    explained_variance   | 0.128      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.174      |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    value_loss           | 0.508      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 40.4        |\n",
      "|    ep_rew_mean          | 5.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013429526 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.627      |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.284       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.595       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 48.8        |\n",
      "|    ep_rew_mean          | 6.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 824         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010087957 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 0.536       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 0.605       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62.2        |\n",
      "|    ep_rew_mean          | 7.93        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007589575 |\n",
      "|    clip_fraction        | 0.0925      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.617       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 0.545       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 76.7        |\n",
      "|    ep_rew_mean          | 9.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 794         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010405503 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 0.344       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 91.2      |\n",
      "|    ep_rew_mean          | 10.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 787       |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 20        |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0129163 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.557    |\n",
      "|    explained_variance   | 0.873     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.091     |\n",
      "|    n_updates            | 70        |\n",
      "|    policy_gradient_loss | -0.0155   |\n",
      "|    value_loss           | 0.368     |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 108          |\n",
      "|    ep_rew_mean          | 12.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 795          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062898854 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.577       |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0982       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00754     |\n",
      "|    value_loss           | 0.266        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 118        |\n",
      "|    ep_rew_mean          | 13.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00599509 |\n",
      "|    clip_fraction        | 0.0452     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.555     |\n",
      "|    explained_variance   | 0.866      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0441     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00729   |\n",
      "|    value_loss           | 0.236      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 135         |\n",
      "|    ep_rew_mean          | 16.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 787         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007567265 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.541      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0345      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.0778      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 153         |\n",
      "|    ep_rew_mean          | 18.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 784         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005785732 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00292     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 0.0734      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 171         |\n",
      "|    ep_rew_mean          | 21.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 788         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006116289 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00714    |\n",
      "|    value_loss           | 0.0463      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 190         |\n",
      "|    ep_rew_mean          | 23.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 793         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003451875 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.484      |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00496     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00371    |\n",
      "|    value_loss           | 0.0242      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 207          |\n",
      "|    ep_rew_mean          | 26.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 797          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023971987 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.663        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0113      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00335     |\n",
      "|    value_loss           | 0.0181       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 224          |\n",
      "|    ep_rew_mean          | 28.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 798          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042629754 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.436       |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00574     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 0.0119       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 241          |\n",
      "|    ep_rew_mean          | 31           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 798          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039764335 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.416       |\n",
      "|    explained_variance   | 0.6          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00625      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    value_loss           | 0.00884      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 258          |\n",
      "|    ep_rew_mean          | 33.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 799          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033087218 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.401       |\n",
      "|    explained_variance   | 0.68         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000545     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 0.0055       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 280         |\n",
      "|    ep_rew_mean          | 36.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004603082 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.371      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00311    |\n",
      "|    value_loss           | 0.00462     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 298         |\n",
      "|    ep_rew_mean          | 39.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002362304 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.351      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0134      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00302    |\n",
      "|    value_loss           | 0.00377     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 316          |\n",
      "|    ep_rew_mean          | 41.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 802          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016299523 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.32        |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00529      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 0.00238      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 333          |\n",
      "|    ep_rew_mean          | 44.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 800          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020918208 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.327       |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.015       |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00434     |\n",
      "|    value_loss           | 0.00216      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 354          |\n",
      "|    ep_rew_mean          | 47.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 799          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020753252 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.306       |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0136      |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 0.00155      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 371          |\n",
      "|    ep_rew_mean          | 49.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 796          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016767483 |\n",
      "|    clip_fraction        | 0.0273       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.293       |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00393     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00299     |\n",
      "|    value_loss           | 0.0013       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 386         |\n",
      "|    ep_rew_mean          | 52.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 797         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007983596 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000938   |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00282    |\n",
      "|    value_loss           | 0.000749    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# 2. 定义一个 Wrapper，在 step 里用你的 MLP 计算 reward\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, reward_model_path, device=\"cpu\"):\n",
    "        super().__init__(env)\n",
    "        # 创建并加载你的 MLP 奖励模型\n",
    "\n",
    "        # state 维度\n",
    "        self.dim_state = env.observation_space.shape[0]\n",
    "\n",
    "        # action 维度\n",
    "        try:                  self.dim_action = env.action_space.shape[0]\n",
    "        except IndexError:    self.dim_action = 1\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Ensure the input dimensions match the checkpoint\n",
    "        checkpoint = torch.load(reward_model_path, map_location=device, weights_only=False)\n",
    "        input_dim = checkpoint['net.0.weight'].size(1)  # Extract input size from checkpoint\n",
    "        self.reward_model = RewardMLP(input_dim - self.dim_action, self.dim_action).to(device)\n",
    "        self.reward_model.load_state_dict(checkpoint)\n",
    "        self.reward_model.load_state_dict(torch.load(reward_model_path, map_location=device, weights_only=False))\n",
    "        self.reward_model.eval()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 执行原 env，不用原 reward\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)   \n",
    "        \n",
    "        # 转成 batch 形式再丢给网络\n",
    "        state_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.Discrete):\n",
    "            action_tensor = torch.tensor([action], dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        # 修改 action_tensor 形状\n",
    "        if action_tensor.ndim == 1:\n",
    "            action_tensor = action_tensor.view(1, -1)\n",
    "\n",
    "        # 计算奖励\n",
    "        with torch.no_grad():\n",
    "            reward_tensor = self.reward_model(state_tensor, \n",
    "                                              action_tensor)\n",
    "        reward = reward_tensor.item()\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "def reset(self, **kwargs):\n",
    "    # 接受 seed, options, return_info 等任意参数\n",
    "    return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# 3. 构造 vectorized 环境，并应用自定义 Wrapper\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Log 地址\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_path = os.path.join(\"Training\", current_time)\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "# Reward Model 地址\n",
    "MODEL_PATH = \"reward_net.pth\"\n",
    "\n",
    "# 如果你用 SB3 的 make_vec_env，可以传 wrapper_class\n",
    "vec_env = make_vec_env(\n",
    "    env_id=\"CartPole-v1\",\n",
    "    n_envs=8,\n",
    "    wrapper_class=lambda env: CustomRewardWrapper(env, MODEL_PATH, device=\"cpu\"),\n",
    "    monitor_dir=log_path\n",
    ")\n",
    "# 再加一个 Monitor（记录 episode reward 到文件）\n",
    "vec_env = VecMonitor(vec_env, log_path)\n",
    "\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    n_steps=256,\n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_path\n",
    ")\n",
    "\n",
    "model.learn( \n",
    "    total_timesteps=50000,\n",
    "    # callback=[eval_callback, save_callback]\n",
    ")\n",
    "\n",
    "model.save(os.path.join(log_path, \"model_full_training\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01aac6",
   "metadata": {},
   "source": [
    "## **RLHF 模型测试部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c1061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\\2025-05-08_19-59-36\\model_full_training\n",
      "Episode: 1 Score: [500.]\n",
      "Episode: 2 Score: [500.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "pkg_dir = os.path.dirname(cur_dir)\n",
    "\n",
    "if pkg_dir not in sys.path:\n",
    "    sys.path.append(pkg_dir)\n",
    "\n",
    "from Project import tools\n",
    "\n",
    "\n",
    "# 评估模型\n",
    "log_path = \"Training\\\\2025-05-08_19-59-36\"\n",
    "PPO_Model_Path = os.path.join(log_path, \"model_full_training\")\n",
    "tools.test_model(\"PPO\", PPO_Model_Path, n_episodes=2, render = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
