{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e72df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514fce1",
   "metadata": {},
   "source": [
    "## **User Define Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5849e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç±»\n",
    "class Data_Class:\n",
    "\n",
    "    # å†…éƒ¨åµŒå¥—ç±»\n",
    "    class Trajectory_Class:\n",
    "        def __init__(self, traj_series):\n",
    "            self.traj_list = traj_series\n",
    "            self.length = len(traj_series)\n",
    "\n",
    "        def get_single_traj(self, index):\n",
    "            return json.loads(self.traj_list[index])\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        # åŸå§‹æ•°æ®\n",
    "        self.trajs_prefer_list = []\n",
    "        self.trajs_reject_list = []\n",
    "\n",
    "        # å¤„ç†æ•°æ®\n",
    "        self.traj_prefer_list_list_tensor = []\n",
    "        self.traj_reject_list_list_tensor = []\n",
    "\n",
    "        # å¯åŠ¨å‡½æ•°\n",
    "        self.load_data(path)\n",
    "        self.convert(self.trajs_prefer_list, self.traj_prefer_list_list_tensor) # æ•°æ®è½¬æ¢\n",
    "        self.convert(self.trajs_reject_list, self.traj_reject_list_list_tensor)\n",
    "        print(\"Data loaded successfully\")\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        self.trajs_prefer_list = Data_Class.Trajectory_Class(data['preferred'])   # list æ•°æ®\n",
    "        self.trajs_reject_list = Data_Class.Trajectory_Class(data['rejected'])    # list æ•°æ®\n",
    "\n",
    "    def convert(self,\n",
    "                list_json: Trajectory_Class,\n",
    "                traj_list_list_tensor):\n",
    "\n",
    "        # è·å–ç¬¬0æ¡è½¨è¿¹çš„ç¬¬0æ—¶åˆ»æ ·æœ¬æ¥ç¡®å®šç»´åº¦\n",
    "        sample = list_json.get_single_traj(0)[0]\n",
    "        state0 = np.array(sample['state'])\n",
    "        action0 = np.array(sample['action'])\n",
    "\n",
    "        # è·å– state action ç»´åº¦\n",
    "        self.dim_state = state0.size if state0.ndim == 0 else state0.shape[0]\n",
    "        self.dim_action = action0.size if action0.ndim == 0 else action0.shape[0]\n",
    "\n",
    "        # æ•°æ®æ‰¹é‡è½¬æ¢ tensor\n",
    "        for idx in range(list_json.length):\n",
    "            traj = list_json.get_single_traj(idx)\n",
    "            states, actions = [], []\n",
    "\n",
    "            for time_i in traj:\n",
    "                # è½¬æ¢ä¸º numpyï¼Œç„¶å torch tensor\n",
    "                state_np = np.array(time_i['state'])\n",
    "                action_np = np.array(time_i['action'])\n",
    "\n",
    "                state_t = torch.from_numpy(state_np).float()\n",
    "                action_t = torch.from_numpy(action_np).float()\n",
    "\n",
    "                # å¦‚æœæ˜¯ä¸€ç»´æ ‡é‡ï¼Œè¦å±•å¼€æˆé•¿åº¦1å‘é‡\n",
    "                state_t = state_t.view(-1)\n",
    "                action_t = action_t.view(-1)\n",
    "\n",
    "                states.append(state_t)\n",
    "                actions.append(action_t)\n",
    "\n",
    "            # å°†åˆ—è¡¨å †æˆå¼ é‡ [L_i, dim]\n",
    "            states_tensor = torch.stack(states, dim=0)\n",
    "            actions_tensor = torch.stack(actions, dim=0)\n",
    "\n",
    "            # å°†æ¯æ¡è½¨è¿¹ä½œä¸ºä¸€ä¸ªå…ƒç»„ (states, actions) æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
    "            traj_list_list_tensor.append((states_tensor, actions_tensor))\n",
    "\n",
    "# â€”â€”â€” æ•°æ®é›†ä¸åŠ è½½å™¨ â€”â€”â€”\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, pref, rej, gamma):\n",
    "        assert len(pref) == len(rej)\n",
    "        self.pref = pref\n",
    "        self.rej = rej\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pref)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (*self.pref[idx], *self.rej[idx])\n",
    "\n",
    "# åˆ›å»º MLP æ‰“åˆ†æ¨¡å‹ #0000FF\n",
    "class RewardMLP(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim + a_dim, hidden_dim),  # è¿™é‡Œåœ¨æ„é€ ç¥ç»ç½‘ç»œ\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        # s: [L_i, s_dim], a: [L_i, a_dim]\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8606d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "0 200 200\n",
      "1 200 144\n",
      "2 200 180\n",
      "3 200 194\n",
      "4 200 96\n",
      "5 200 125\n",
      "6 200 108\n",
      "7 200 106\n",
      "8 200 166\n",
      "9 173 97\n"
     ]
    }
   ],
   "source": [
    "# â€”â€”â€”â€” åŠ è½½æ•°æ® â€”â€”â€”â€”\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "for i in range(10):\n",
    "    print(i, len(Data.trajs_prefer_list.get_single_traj(i)), len(Data.trajs_reject_list.get_single_traj(i)))\n",
    "\n",
    "# print(len(Data.trajs_prefer_list.get_single_traj(0)),  len(Data.trajs_reject_list.get_single_traj(0)))\n",
    "# print(len(Data.trajs_prefer_list.get_single_traj(1)),  len(Data.trajs_reject_list.get_single_traj(1)))\n",
    "# print(len(Data.trajs_prefer_list.get_single_traj(2)),  len(Data.trajs_reject_list.get_single_traj(2)))\n",
    "\n",
    "# print(\"\")\n",
    "# print(len(Data.traj_prefer_list_list_tensor[0][0]), len(Data.traj_reject_list_list_tensor[0][0]))\n",
    "# print(len(Data.traj_prefer_list_list_tensor[1][0]), len(Data.traj_reject_list_list_tensor[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eeca22",
   "metadata": {},
   "source": [
    "## **Reward Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b87a9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Epoch 1/50 â€” Avg Loss: 1.8811\n",
      "Epoch 2/50 â€” Avg Loss: 1.5743\n",
      "Epoch 3/50 â€” Avg Loss: 1.3052\n",
      "Epoch 4/50 â€” Avg Loss: 1.0893\n",
      "Epoch 5/50 â€” Avg Loss: 0.9303\n",
      "Epoch 6/50 â€” Avg Loss: 0.7888\n",
      "Epoch 7/50 â€” Avg Loss: 0.6872\n",
      "Epoch 8/50 â€” Avg Loss: 0.5927\n",
      "Epoch 9/50 â€” Avg Loss: 0.5236\n",
      "Epoch 10/50 â€” Avg Loss: 0.4684\n",
      "Epoch 11/50 â€” Avg Loss: 0.4259\n",
      "Epoch 12/50 â€” Avg Loss: 0.3925\n",
      "Epoch 13/50 â€” Avg Loss: 0.3677\n",
      "Epoch 14/50 â€” Avg Loss: 0.3490\n",
      "Epoch 15/50 â€” Avg Loss: 0.3347\n",
      "Epoch 16/50 â€” Avg Loss: 0.3233\n",
      "Epoch 17/50 â€” Avg Loss: 0.3135\n",
      "Epoch 18/50 â€” Avg Loss: 0.3075\n",
      "Epoch 19/50 â€” Avg Loss: 0.3004\n",
      "Epoch 20/50 â€” Avg Loss: 0.2953\n",
      "Epoch 21/50 â€” Avg Loss: 0.2907\n",
      "Epoch 22/50 â€” Avg Loss: 0.2864\n",
      "Epoch 23/50 â€” Avg Loss: 0.2825\n",
      "Epoch 24/50 â€” Avg Loss: 0.2795\n",
      "Epoch 25/50 â€” Avg Loss: 0.2760\n",
      "Epoch 26/50 â€” Avg Loss: 0.2737\n",
      "Epoch 27/50 â€” Avg Loss: 0.2711\n",
      "Epoch 28/50 â€” Avg Loss: 0.2686\n",
      "Epoch 29/50 â€” Avg Loss: 0.2666\n",
      "Epoch 30/50 â€” Avg Loss: 0.2649\n",
      "Epoch 31/50 â€” Avg Loss: 0.2622\n",
      "Epoch 32/50 â€” Avg Loss: 0.2613\n",
      "Epoch 33/50 â€” Avg Loss: 0.2587\n",
      "Epoch 34/50 â€” Avg Loss: 0.2587\n",
      "Epoch 35/50 â€” Avg Loss: 0.2568\n",
      "Epoch 36/50 â€” Avg Loss: 0.2547\n",
      "Epoch 37/50 â€” Avg Loss: 0.2533\n",
      "Epoch 38/50 â€” Avg Loss: 0.2521\n",
      "Epoch 39/50 â€” Avg Loss: 0.2505\n",
      "Epoch 40/50 â€” Avg Loss: 0.2493\n",
      "Epoch 41/50 â€” Avg Loss: 0.2488\n",
      "Epoch 42/50 â€” Avg Loss: 0.2470\n",
      "Epoch 43/50 â€” Avg Loss: 0.2460\n",
      "Epoch 44/50 â€” Avg Loss: 0.2455\n",
      "Epoch 45/50 â€” Avg Loss: 0.2437\n",
      "Epoch 46/50 â€” Avg Loss: 0.2430\n",
      "Epoch 47/50 â€” Avg Loss: 0.2415\n",
      "Epoch 48/50 â€” Avg Loss: 0.2406\n",
      "Epoch 49/50 â€” Avg Loss: 0.2403\n",
      "Epoch 50/50 â€” Avg Loss: 0.2395\n",
      "ğŸ‰ åˆæˆæ•°æ®ä¸Šçš„å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆï¼\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ° reward_net.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â€”â€”â€”â€” è¶…å‚æ•° â€”â€”â€”â€”\n",
    "num_pairs = 200    # åå¥½å¯¹æ•°é‡\n",
    "T = 50             # æœŸæœ›æœ€å¤§è½¨è¿¹é•¿åº¦ï¼ˆç”¨äºè¯„ä¼°æˆ–å…¶ä»–éœ€æ±‚ï¼‰\n",
    "s_dim = 4         # çŠ¶æ€ç»´åº¦ [è§’åº¦, è§’é€Ÿåº¦, å°è½¦ä½ç½®, å°è½¦é€Ÿåº¦]\n",
    "a_dim = 1         # åŠ¨ä½œç»´åº¦ï¼ˆæ¨åŠ›ï¼‰\n",
    "gamma = 0.99      # æŠ˜æ‰£å› å­\n",
    "lr = 1e-4         # å­¦ä¹ ç‡\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "\n",
    "# â€”â€”â€”â€” åŠ è½½æ•°æ® â€”â€”â€”â€”\n",
    "path = \"trajectory_pairs.csv\"\n",
    "Data = Data_Class(path)\n",
    "\n",
    "# è‡ªå®šä¹‰ collate_fnï¼Œä¿ç•™å˜é•¿åºåˆ—\n",
    "def variable_collate(batch):\n",
    "    # batch: List of tuples (s_pref, a_pref, s_rej, a_rej)\n",
    "    s_pf, a_pf, s_rj, a_rj = zip(*batch)\n",
    "    return list(s_pf), list(a_pf), list(s_rj), list(a_rj)\n",
    "\n",
    "# å‡†å¤‡è®­ç»ƒ\n",
    "dataset = PreferenceDataset(\n",
    "    Data.traj_prefer_list_list_tensor,\n",
    "    Data.traj_reject_list_list_tensor,\n",
    "    gamma\n",
    ")\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=variable_collate\n",
    ")\n",
    "\n",
    "reward_net = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64) # å®ä¾‹åŒ– ç¥ç»ç½‘ç»œ MLP\n",
    "optimizer  = optim.Adam(reward_net.parameters(), lr=lr)\n",
    "loss_fn    = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# â€”â€”â€” è®­ç»ƒå¾ªç¯ â€”â€”â€”\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for s_pref_list, a_pref_list, s_rej_list, a_rej_list in loader:\n",
    "        R_pref_batch = []\n",
    "        R_rej_batch  = []\n",
    "\n",
    "        # è®¡ç®— prefer è½¨è¿¹çš„å›æŠ¥\n",
    "        for s_pf, a_pf in zip(s_pref_list, a_pref_list):\n",
    "            r_pf = reward_net(s_pf, a_pf)           # [L_i]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_pf.size(0))], device=r_pf.device)\n",
    "            R_pref_batch.append((r_pf * discounts).sum())\n",
    "\n",
    "        # è®¡ç®— reject è½¨è¿¹çš„å›æŠ¥\n",
    "        for s_rj, a_rj in zip(s_rej_list, a_rej_list):\n",
    "            r_rj = reward_net(s_rj, a_rj)          # [L_j]\n",
    "            discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))], device=r_rj.device)\n",
    "            R_rej_batch.append((r_rj * discounts).sum())\n",
    "\n",
    "        R_pref = torch.stack(R_pref_batch)\n",
    "        R_rej = torch.stack(R_rej_batch)\n",
    "\n",
    "        logits = R_pref - R_rej\n",
    "        targets = torch.ones_like(logits)        # pref åº”å¾—æ›´é«˜åˆ†\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(R_pref_batch)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} â€” Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"ğŸ‰ åˆæˆæ•°æ®ä¸Šçš„å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n",
    "\n",
    "# â€”â€”â€” ä¿å­˜æ¨¡å‹ â€”â€”â€”\n",
    "torch.save(reward_net.state_dict(), 'reward_net.pth')\n",
    "print(\"æ¨¡å‹å·²ä¿å­˜åˆ° reward_net.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6bec",
   "metadata": {},
   "source": [
    "## **Reward Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "076e2acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½å¹¶å‡†å¤‡å¥½è¿›è¡Œæ¨ç†\n",
      "0 tensor(10.9267) tensor(9.9196)\n",
      "1 tensor(9.7878) tensor(7.5548)\n",
      "2 tensor(11.4603) tensor(9.7593)\n",
      "3 tensor(10.9521) tensor(9.6708)\n",
      "4 tensor(11.2456) tensor(5.3581)\n",
      "5 tensor(11.0701) tensor(6.5134)\n",
      "6 tensor(10.5024) tensor(5.3424)\n",
      "7 tensor(10.8763) tensor(5.1974)\n",
      "8 tensor(10.2842) tensor(9.6710)\n",
      "9 tensor(10.4395) tensor(6.1724)\n"
     ]
    }
   ],
   "source": [
    "# â€”â€”â€” åŠ è½½æ¨¡å‹ç¤ºä¾‹ â€”â€”â€”\n",
    "reward_net_loaded = RewardMLP(Data.dim_state, Data.dim_action, hidden_dim=64)\n",
    "reward_net_loaded.load_state_dict(torch.load('reward_net.pth', weights_only=True))\n",
    "\n",
    "# åˆ‡åˆ°æ¨ç†æ¨¡å¼ï¼Œå¹¶å…³é—­æ¢¯åº¦\n",
    "reward_net_loaded.eval()\n",
    "print(\"åŠ è½½å¹¶å‡†å¤‡å¥½è¿›è¡Œæ¨ç†\")\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "for i in range(10):\n",
    "    traj_prefer_json = Data.trajs_prefer_list.get_single_traj(i)\n",
    "    traj_reject_json = Data.trajs_reject_list.get_single_traj(i)\n",
    "\n",
    "    # æŠŠå®ƒè½¬æˆå¼ é‡\n",
    "    states_prefer  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                  for step in traj_prefer_json], dim=0)  # [L, s_dim]\n",
    "    actions_prefer = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_prefer_json], dim=0)  # [L, a_dim]\n",
    "    states_reject  = torch.stack([torch.from_numpy(np.array(step['state'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, s_dim]\n",
    "    actions_reject = torch.stack([torch.from_numpy(np.array(step['action'])).float().view(-1)\n",
    "                                    for step in traj_reject_json], dim=0)  # [L, a_dim]\n",
    "\n",
    "    # è®¡ç®— prefer è½¨è¿¹çš„å›æŠ¥\n",
    "    with torch.no_grad():\n",
    "        r_pref = reward_net_loaded(states_prefer, actions_prefer)           # [L_i]\n",
    "\n",
    "    # è®¡ç®—æ€»å›æŠ¥\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_pref.size(0))])\n",
    "    total_return_prefer = (r_pref * discounts).sum()\n",
    "\n",
    "    # è®¡ç®— reject è½¨è¿¹çš„å›æŠ¥\n",
    "    with torch.no_grad():\n",
    "        r_rj = reward_net_loaded(states_reject, actions_reject)          # [L_j]\n",
    "\n",
    "    # è®¡ç®—æ€»å›æŠ¥\n",
    "    discounts = torch.tensor([gamma**t for t in range(r_rj.size(0))])\n",
    "    total_return_reject = (r_rj * discounts).sum()\n",
    "\n",
    "    print(i, total_return_prefer, total_return_reject)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79951354",
   "metadata": {},
   "source": [
    "## **RLHF æ¨¡å‹è®­ç»ƒ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41f50c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 2. å®šä¹‰ä¸€ä¸ª Wrapperï¼Œåœ¨ step é‡Œç”¨ä½ çš„ MLP è®¡ç®— reward\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomRewardWrapper\u001b[39;00m(\u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mWrapper):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env, reward_model_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# 2. å®šä¹‰ä¸€ä¸ª Wrapperï¼Œåœ¨ step é‡Œç”¨ä½ çš„ MLP è®¡ç®— reward\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, reward_model_path, device=\"cpu\"):\n",
    "        super().__init__(env)\n",
    "        # åˆ›å»ºå¹¶åŠ è½½ä½ çš„ MLP å¥–åŠ±æ¨¡å‹\n",
    "\n",
    "        # state ç»´åº¦\n",
    "        self.dim_state = env.observation_space.shape[0]\n",
    "\n",
    "        # action ç»´åº¦\n",
    "        try:                  self.dim_action = env.action_space.shape[0]\n",
    "        except IndexError:    self.dim_action = 1\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Ensure the input dimensions match the checkpoint\n",
    "        checkpoint = torch.load(reward_model_path, map_location=device, weights_only=False)\n",
    "        input_dim = checkpoint['net.0.weight'].size(1)  # Extract input size from checkpoint\n",
    "        self.reward_model = RewardMLP(input_dim - self.dim_action, self.dim_action).to(device)\n",
    "        self.reward_model.load_state_dict(checkpoint)\n",
    "        self.reward_model.load_state_dict(torch.load(reward_model_path, map_location=device, weights_only=False))\n",
    "        self.reward_model.eval()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # æ‰§è¡ŒåŸ envï¼Œä¸ç”¨åŸ reward\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)   \n",
    "        \n",
    "        # è½¬æˆ batch å½¢å¼å†ä¸¢ç»™ç½‘ç»œ\n",
    "        state_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.Discrete):\n",
    "            action_tensor = torch.tensor([action], dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        # ä¿®æ”¹ action_tensor å½¢çŠ¶\n",
    "        if action_tensor.ndim == 1:\n",
    "            action_tensor = action_tensor.view(1, -1)\n",
    "\n",
    "        # è®¡ç®—å¥–åŠ±\n",
    "        with torch.no_grad():\n",
    "            reward_tensor = self.reward_model(state_tensor, \n",
    "                                              action_tensor)\n",
    "        reward = reward_tensor.item()\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "def reset(self, **kwargs):\n",
    "    # æ¥å— seed, options, return_info ç­‰ä»»æ„å‚æ•°\n",
    "    return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# 3. æ„é€  vectorized ç¯å¢ƒï¼Œå¹¶åº”ç”¨è‡ªå®šä¹‰ Wrapper\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Log åœ°å€\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_path = os.path.join(\"Training\", current_time)\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "# Reward Model åœ°å€\n",
    "MODEL_PATH = \"reward_net.pth\"\n",
    "\n",
    "# å¦‚æœä½ ç”¨ SB3 çš„ make_vec_envï¼Œå¯ä»¥ä¼  wrapper_class\n",
    "vec_env = make_vec_env(\n",
    "    env_id=\"CartPole-v1\",\n",
    "    n_envs=8,\n",
    "    wrapper_class=lambda env: CustomRewardWrapper(env, MODEL_PATH, device=\"cpu\"),\n",
    "    monitor_dir=log_path\n",
    ")\n",
    "# å†åŠ ä¸€ä¸ª Monitorï¼ˆè®°å½• episode reward åˆ°æ–‡ä»¶ï¼‰\n",
    "vec_env = VecMonitor(vec_env, log_path)\n",
    "\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=vec_env,\n",
    "    n_steps=256,\n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_path\n",
    ")\n",
    "\n",
    "model.learn( \n",
    "    total_timesteps=50000,\n",
    "    # callback=[eval_callback, save_callback]\n",
    ")\n",
    "\n",
    "model.save(os.path.join(log_path, \"model_full_training\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01aac6",
   "metadata": {},
   "source": [
    "## **RLHF æ¨¡å‹æµ‹è¯•éƒ¨åˆ†**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\\2025-05-08_19-59-36\\model_full_training\n",
      "Episode: 1 Score: [500.]\n",
      "Episode: 2 Score: [500.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "pkg_dir = os.path.dirname(cur_dir)\n",
    "\n",
    "if pkg_dir not in sys.path:\n",
    "    sys.path.append(pkg_dir)\n",
    "\n",
    "from Project import tools\n",
    "\n",
    "\n",
    "# è¯„ä¼°æ¨¡å‹\n",
    "log_path = \"Training\\\\2025-05-08_19-59-36\"\n",
    "PPO_Model_Path = os.path.join(log_path, \"model_full_training\")\n",
    "tools.test_model(\"PPO\", PPO_Model_Path, n_episodes=2, render = True, record=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
